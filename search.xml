<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BitNet_paper</title>
      <link href="/2025/06/30/BitNet-paper/"/>
      <url>/2025/06/30/BitNet-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="BitNet解读"><a href="#BitNet解读" class="headerlink" title="BitNet解读"></a>BitNet解读</h1><p>这是论文最核心的创新。不同于传统的16位浮点数权重，也不同于后训练量化（如4-bit, 2-bit）或之前的1-bit二值（±1）模型，BitNet b1.58 首次成功地将LLM的<strong>所有权重</strong>约束到仅取 <code>&#123;-1, 0, +1&#125;</code> 三个值。这个“0”的引入是关键突破。</p><img src="image-20250630170530844.png" alt="image-20250630170530844" style="zoom:67%;" /><h2 id="核心思想与核心贡献（创新点）"><a href="#核心思想与核心贡献（创新点）" class="headerlink" title="核心思想与核心贡献（创新点）"></a>核心思想与核心贡献（创新点）</h2><h3 id="三值-1-58-bit-权重量化"><a href="#三值-1-58-bit-权重量化" class="headerlink" title="三值 (1.58-bit) 权重量化"></a>三值 (1.58-bit) 权重量化</h3><p>这是论文最核心的创新。不同于传统的16位浮点数权重，也不同于后训练量化（如4-bit, 2-bit）或之前的1-bit二值（±1）模型，BitNet b1.58 首次成功地将LLM的<strong>所有权重</strong>约束到仅取 <code>&#123;-1, 0, +1&#125;</code> 三个值。这个“0”的引入是关键突破。</p><ul><li><strong>为什么是1.58比特？</strong> 因为用二进制表示3个状态 (<code>-1</code>, <code>0</code>, <code>1</code>) 理论上需要 <code>log2(3) ≈ 1.58</code> 比特。这比16-bit FP16小了约10倍，比8-bit INT8小了约5倍，比4-bit小了约2.5倍。</li><li><strong>“0”的重要性：</strong> 引入零值 (<code>0</code>) 提供了<strong>显式的特征过滤能力</strong>。权重为0意味着该连接被完全关闭，这赋予了模型更强的表达能力和稀疏性（虽然是非结构化稀疏），显著提升了1-bit级别模型的建模能力，使其最终能达到FP16基线的性能。</li></ul><h3 id="性能匹配全精度模型"><a href="#性能匹配全精度模型" class="headerlink" title="性能匹配全精度模型"></a>性能匹配全精度模型</h3><p>论文通过严谨的实验证明，<strong>从3B模型规模开始</strong>，BitNet b1.58 在<strong>困惑度 (Perplexity, PPL)</strong> 和各种<strong>下游任务 (End-task Performance)</strong> 的<strong>零样本 (Zero-shot)</strong> 准确率上，能够<strong>匹配甚至略微超越</strong>相同模型大小、相同训练数据量的全精度（FP16）LLaMA基线模型。这是低比特模型发展史上的一个里程碑，证明了1.58-bit模型在保持高性能上的可行性。</p><h3 id="革命性的效率提升"><a href="#革命性的效率提升" class="headerlink" title="革命性的效率提升"></a>革命性的效率提升</h3><p> BitNet b1.58 带来了全方位的效率优势，这是其最具吸引力的地方：</p><ul><li><p><strong>计算效率 (Latency)：</strong> 矩阵乘法 (<code>nn.Linear</code>) 是LLM计算的核心。FP16模型需要昂贵的浮点乘加运算 (FP16 Multiply-Add)。而BitNet b1.58 的权重是<code>&#123;-1, 0, +1&#125;</code>，其矩阵乘主要转化为**整数加法 (INT8 Addition)**：</p><ul><li><p>权重<code>-1</code>或<code>+1</code>：只需对激活值进行累加或累减（本质是加法）。</p></li><li><p>权重<code>0</code>：直接跳过计算。</p></li><li><p><strong>几乎消除了乘法操作！</strong> 这在硬件层面极其高效。</p><img src="image-20250629222219371.png" alt="image-20250629222219371" style="zoom:80%;" /><p>如图3所示，在7nm工艺下，其矩阵乘算术操作能耗比FP16低 <strong>7.14倍</strong>。如表1所示，3B BitNet的推理延迟比3B FP16 LLaMA <strong>快2.71倍</strong>。</p><img src="image-20250629222159576.png" alt="image-20250629222159576" style="zoom: 80%;" /><p>图2显示，随着模型增大（到70B），速度优势更加显著（<strong>快4.1倍</strong>）。</p></li></ul></li><li><p><strong>内存效率 (Memory)：</strong> 三值权重本身存储开销极小（1.58 bits&#x2F;weight）。更重要的是，<strong>降低了内存带宽需求</strong>。加载权重从DRAM到计算单元（如SRAM）是推理瓶颈。BitNet b1.58 极小的权重位宽显著减少了数据传输量和时间。</p><img src="image-20250629230325191.png" alt="image-20250629230325191" style="zoom: 67%;" /><p>表1所示，3B BitNet的GPU内存占用比FP16 LLaMA的GPU内存减少了72%。</p><img src="image-20250629230413011.png" alt="image-20250629230413011" style="zoom:80%;" /><p>图2显示内存节省随模型增大而增加。</p></li><li><p><strong>吞吐量 (Throughput)：</strong> 低内存占用和高计算效率共同作用，使得BitNet b1.58 在固定硬件（如GPU）上能运行<strong>更大的批次，即Batch Size</strong>。</p><img src="image-20250629230446638.png" alt="image-20250629230446638" style="zoom:67%;" /><p>表3所示，70B模型在两块A100上，BitNet b1.58 的最大批次大小是FP16 LLaMA的 <strong>11倍</strong>，最终吞吐量达到 <strong>8.9倍</strong>。</p></li><li><p><strong>能耗 (Energy)：</strong> 计算和内存效率的提升直接转化为能耗的显著降低。</p><img src="image-20250629230542985.png" alt="image-20250629230542985" style="zoom:80%;" /><p>图3所示，BitNet b1.58 的端到端能耗远低于FP16基线，且模型越大，优势越明显（70B BitNet能耗远低于13B FP16 LLaMA）。</p></li></ul><h3 id="新的缩放定律"><a href="#新的缩放定律" class="headerlink" title="新的缩放定律"></a>新的缩放定律</h3><p>论文第5页（图2, 图3及文字）提出了基于BitNet b1.58效率优势的<strong>新模型缩放等价关系</strong>：</p><ul><li>13B BitNet b1.58 在延迟、内存和能耗上比 3B FP16 LLM 更高效。</li><li>30B BitNet b1.58 比 7B FP16 LLM 更高效。</li><li>70B BitNet b1.58 比 13B FP16 LLM 更高效。</li><li>这意味着，为了达到特定的性能水平，<strong>选择训练一个更大的BitNet b1.58模型，相比训练一个更小的FP16模型，在推理时成本更低、速度更快</strong>。这颠覆了传统“更大模型必然更慢更耗能”的认知，为未来高效大模型的设计提供了新的指导原则。</li></ul><h3 id="新的计算范式-专用硬件呼吁"><a href="#新的计算范式-专用硬件呼吁" class="headerlink" title="新的计算范式 &amp; 专用硬件呼吁"></a>新的计算范式 &amp; 专用硬件呼吁</h3><p>BitNet b1.58 的计算核心是<strong>基于三值权重的整数加法</strong>，这与传统FP&#x2F;INT乘加计算截然不同。论文（引言、图1、第6页）强烈呼吁，这种新范式为设计<strong>专门优化1-bit (或极低比特) LLM的硬件</strong>（如新型AI加速器，文中提到类似Groq的LPU概念）打开了大门。这类硬件可以彻底移除昂贵的乘法器，专注于超高效的整数加法阵列和超宽低精度数据通路，潜力巨大。</p><h2 id="论文其他重要部分的深入解释"><a href="#论文其他重要部分的深入解释" class="headerlink" title="论文其他重要部分的深入解释"></a>论文其他重要部分的深入解释</h2><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><ul><li><strong>基础：</strong> 基于Transformer，核心是用 <code>BitLinear</code> 层替换标准的 <code>nn.Linear</code> (全连接层)。</li><li><strong>权重量化函数 (Quantization Function - Key Detail!):</strong> 这是实现三值权重的核心算法。<ul><li>采用 <code>absmean</code> 量化：<code>γ = (1/nm) * Σ|W_ij|</code> (公式3), 即权重矩阵绝对值的平均值。</li><li>归一化：<code>W&#39; = W / (γ + ε)</code> (ε 防止除零)。</li><li>三值化：<code>W̃ = RoundClip(W&#39;, -1, 1)</code> (公式1, 2)。<code>RoundClip</code> 将值约束在[-1,1]区间并四舍五入到最近的整数 (<code>-1</code>, <code>0</code>, <code>+1</code>)。这个过程在<strong>训练</strong>中应用（Straight-Through Estimator, STE 用于梯度回传，文中虽未详述但应如此）。</li></ul></li><li><strong>激活量化 (Activation Quantization):</strong> 激活值量化为<strong>8-bit整数 (INT8)<strong>。与原始BitNet不同，它将</strong>每个token的激活值统一缩放到 <code>[-Qb, Qb]</code> 范围</strong>。这样做是为了<strong>消除零点 (Zero-Point)</strong> 量化（常见于非对称量化如INT8），简化了实现和系统优化，且实验证明对性能影响可忽略。</li><li><strong>组件兼容性 (LLaMA-alike Components):</strong> 为了无缝融入开源生态（Hugging Face, vLLM, llama.cpp），BitNet b1.58 采用了LLaMA的标准组件：RMSNorm, SwiGLU, Rotary Embeddings, 并移除了所有偏置 (Biases)。这大大提升了其可用性和部署便利性。</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>这是论文的实证核心，用大量数据支撑前述创新点。</p><ul><li><p><strong>设置：</strong> 在RedPajama数据集上预训练100B tokens。对比模型是作者复现的FP16 LLaMA。</p></li><li><p><strong>性能指标：</strong></p><ul><li><p><strong>困惑度 (PPL):</strong> </p><p><code>PPL源于信息论中的概念，它衡量的是语言模型分配给一个测试序列的概率分布的“质量”。其直观意义是模型在预测下一个词时，它有多少个“等可能的候选词”需要犹豫。PPL 的数值大致可以理解为模型在预测下一个词时的平均分支因子。一个PPL=50的模型意味着，在预测每个词时，模型感觉平均有50个 词看起来可能性差不多大（或者说，它在这个“词汇分支树”上平均有50个可能的分支），表明模型对文本序列的预测确定性较低。一个PPL=20的模型则意味着预测每个词时，模型感觉平均只有20个词的可能性差不多大，表明模型的预测确定性更高。</code></p><p><code>PPL值越低越好！ PPL越低，表示语言模型对测试文本序列的预测越准确、越有信心、不确定性越低。</code></p><p>表1显示，<strong>3B BitNet b1.58 的 PPL&#x3D;9.91 优于 3B FP16 LLaMA 的 PPL&#x3D;10.04</strong>。更大的3.9B BitNet (PPL&#x3D;9.62) 优势更明显。</p></li><li><p><strong>下游任务 (Zero-shot Accuracy)</strong></p><img src="image-20250629231049006.png" alt="image-20250629231049006" style="zoom:67%;" /><p>表2显示，在3B规模，BitNet b1.58 在7个任务中的6个上达到或超越FP16 LLaMA，平均分略高 (50.2 vs 49.7)。3.9B BitNet 全面超越3B FP16 LLaMA (51.2 vs 49.7)。以上测试指标均为<strong>准确率</strong></p><table><thead><tr><th align="left">任务简称</th><th align="left">全称</th><th align="left">核心能力评估点</th><th align="left">选项类型</th><th align="left">数据来源&#x2F;特点</th></tr></thead><tbody><tr><td align="left"><strong>ARCe</strong></td><td align="left">ARC-Easy</td><td align="left">基础科学知识、简单推理</td><td align="left">多项选择 (单选)</td><td align="left">小学科学题 (易)</td></tr><tr><td align="left"><strong>ARCc</strong></td><td align="left">ARC-Challenge</td><td align="left">复杂科学推理、深入理解</td><td align="left">多项选择 (单选)</td><td align="left">小学科学题 (难)</td></tr><tr><td align="left"><strong>HS</strong></td><td align="left">HellaSwag</td><td align="left">常识推理、情境建模</td><td align="left">多项选择 (单选)</td><td align="left">日常&#x2F;视频情境续写</td></tr><tr><td align="left"><strong>BQ</strong></td><td align="left">BoolQ</td><td align="left">阅读理解、文本蕴含判断</td><td align="left">二分类 (是&#x2F;否)</td><td align="left">维基百科段落+问题</td></tr><tr><td align="left"><strong>OQ</strong></td><td align="left">OpenbookQA</td><td align="left">知识检索、科学事实推理</td><td align="left">多项选择 (单选)</td><td align="left">开放域科学问题</td></tr><tr><td align="left"><strong>PQ</strong></td><td align="left">PIQA</td><td align="left">物理常识推理</td><td align="left">二选一 (方法选择)</td><td align="left">日常物理操作场景</td></tr><tr><td align="left"><strong>WGe</strong></td><td align="left">Winogrande (XL规模)</td><td align="left">指代消解 (需常识)</td><td align="left">二选一 (名词指代)</td><td align="left">Winograd Schema</td></tr></tbody></table></li></ul></li><li><p><strong>效率指标：</strong> 如前所述，内存、延迟、能耗、吞吐量全方位显著提升，且优势随模型规模增大而放大。图3的能耗分解图直观展示了INT8加法主导的计算如何大幅节省能耗。</p></li><li><p><strong>长训练令牌：</strong></p><img src="image-20250629230940872.png" alt="image-20250629230940872" style="zoom:67%;" /><p>表4展示了BitNet b1.58 3B在2T tokens上训练后，在多个任务上全面超越同样训练2T tokens的最新开源模型 StableLM-3B。这强有力地证明了<strong>1.58-bit LLM不仅能在标准训练量下匹配FP16，在更大数据量下具备更强的泛化能力和潜力</strong>。</p></li></ul><h3 id="未来方向与影响"><a href="#未来方向与影响" class="headerlink" title="未来方向与影响"></a>未来方向与影响</h3><p>论文展望了BitNet b1.58 开启的广阔前景：</p><ul><li><strong>1-bit MoE LLMs:</strong> 混合专家模型(MoE)本身旨在提升效率（计算FLOPs），但其高内存和通信开销是瓶颈。1.58-bit 权重能<strong>极大缓解MoE的内存和跨设备通信负担</strong>，甚至可能实现单芯片部署整个MoE模型。</li><li><strong>原生支持长序列 (Native Long Context):</strong> 处理长序列的关键瓶颈是存储历史信息(KV Cache)的内存。BitNet b1.58 的 <strong>8-bit 激活值</strong> 相比FP16<strong>直接节省一半KV Cache内存</strong>，在相同资源下<strong>上下文长度可翻倍</strong>。论文指出，未来可进一步无损压缩激活值到4-bit或更低，潜力巨大。</li><li><strong>边缘与移动端部署 (Edge &amp; Mobile LLMs):</strong> 极低的<strong>内存占用</strong>和<strong>能耗</strong>，以及对<strong>CPU友好</strong>（主要依赖加法）的特性，使得BitNet b1.58 成为在资源受限的边缘和移动设备上部署强大LLM的理想选择，开启全新的应用场景。</li><li><strong>1-bit LLM 专用硬件 (Dedicated HW):</strong> 再次强调新计算范式（整数加法为主）对设计颠覆性硬件（移除乘法器，超宽低精度数据通路）的呼唤。这可能是实现终极效率的关键。</li></ul><h2 id="总结与评价"><a href="#总结与评价" class="headerlink" title="总结与评价"></a>总结与评价</h2><p>这篇论文标志着大语言模型发展的一个重大转折点，通过其创新的<strong>三值 (1.58-bit) 权重量化方案</strong>，首次在<strong>3B及以上规模</strong>实现了与全精度模型<strong>匹敌的性能</strong>，同时带来了<strong>数量级级别的效率提升</strong>（计算延迟、内存占用、能耗降低、吞吐量提升）。它不仅仅是一个高效的模型变体，更<strong>定义了一种新的缩放定律</strong>（大而高效的1-bit模型优于小而昂贵的FP模型），并<strong>呼唤着新一代硬件计算范式</strong>，其深远影响在于：</p><ol><li><strong>大幅降低LLM的部署和运行成本：</strong> 使更大、更强的模型能在更广泛的设备（从云端到边缘）上经济高效地运行。</li><li><strong>推动LLM的普及化：</strong> 让高性能LLM更易于被研究机构、中小企业甚至个人开发者使用。</li><li><strong>激发硬件创新：</strong> 为AI芯片设计开辟了全新的方向，专注于超低精度、高并行加法运算。</li><li><strong>奠定未来高效LLM基础：</strong> BitNet b1.58 很可能成为未来追求极致效率的LLM研究和应用的基础架构或重要参考。</li></ol>]]></content>
      
      
      <categories>
          
          <category> BitNet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BitNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本地搭建BitNet</title>
      <link href="/2025/05/24/%E6%90%AD%E5%BB%BABitNet/"/>
      <url>/2025/05/24/%E6%90%AD%E5%BB%BABitNet/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>近日，微软发布了一个全新的开源项目——BitNet.cpp，这是专为1-bit大语言模型（LLMs）推理而设计的框架。BitNet.cpp旨在通过优化内核为CPU上运行的1.58-bit模型提供快速且无损的推理支持，并在未来版本中计划支持NPU和GPU。</p><p>BitNet.cpp的开源为1-bit LLM的普及和大规模推理打开了新的大门，其在CPU上的高效推理性能，极大地扩展了大模型在本地设备上的可行性。未来，随着对NPU和GPU的支持，BitNet.cpp有望成为低比特模型推理的主流框架。如果你对大模型在实际应用中的推理性能感兴趣，BitNet.cpp无疑是值得关注和尝试的项目。</p><p>BitNet是微软近期推出的极限精简的推理框架，官方的介绍里，详细介绍了它的架构优势，以及和其他模型的对比实验，总结起来就是不挑设备，不占资源，不减性能！俩字儿牛x，仨字儿很牛x，四个字儿…</p><p>仓库地址：<a href="https://github.com/microsoft/BitNet">https://github.com/microsoft/BitNet</a></p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="C-环境配置"><a href="#C-环境配置" class="headerlink" title="C++环境配置"></a>C++环境配置</h3><p>按照github中的步骤安装相应的C++环境</p><img src="image-20250524151721738.png" alt="image-20250524151721738" style="zoom: 67%;" /><p>当C++相关环境配置完成后，后续的步骤，都需要在<strong>开发者的命令提示符或者PowerShell环境</strong>下进行，这点官方文档也给出了重点说明</p><img src="image-20250524151743221.png" alt="image-20250524151743221" style="zoom:67%;" /><h3 id="BitNet库克隆"><a href="#BitNet库克隆" class="headerlink" title="BitNet库克隆"></a>BitNet库克隆</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/microsoft/BitNet.git</span><br><span class="line">cd BitNet</span><br></pre></td></tr></table></figure><img src="image-20250524152153312.png" alt="image-20250524152153312" /><h3 id="Conda环境配置"><a href="#Conda环境配置" class="headerlink" title="Conda环境配置"></a>Conda环境配置</h3><p>根据步骤提示，创建Conda环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (Recommended) Create a new conda environment</span></span><br><span class="line">conda create -n bitnet-cpp python=<span class="number">3.9</span></span><br><span class="line">conda activate bitnet-cpp</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>安装依赖，直接按文档来即可</p><img src="image-20250524152436022.png" alt="image-20250524152436022" style="zoom: 50%;" /><h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>github文档里介绍的方式是，通过huggingface-cli进行下载，需要科学上网才能正常下载的，大家到这如果进行不下去，可以试试直接到huggingface官网手动把模型下载下来，模型地址：<a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/tree/main%E3%80%82">https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/tree/main。</a></p><p>使用huggingface-cli进行下载如下图所示：</p><img src="image-20250524152415407.png" alt="image-20250524152415407" style="zoom:67%;" /><h3 id="模型编译"><a href="#模型编译" class="headerlink" title="模型编译"></a>模型编译</h3><p>按照github文档说明执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup_env.py -md models/BitNet-b1<span class="number">.58</span>-2B-4T -q i2_s</span><br></pre></td></tr></table></figure><p>这里需要注意的是克隆的bitnet库，那到编译这一步大概率是走不通的，首先是因为这点官方文档里也说明了，由于本项目上游的llama.cpp项目有几个文件存在bug，c++文件丢失了引用，所以我们需要手动的把这几个文件修复一下，官方也给了修复地址在这里：<a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323">https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323</a></p><p>这里我们就直接参照他的说明修改一下就行，不会c++也没问题，分别修复一下四个文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">…\3rdparty\llama.cpp\common\common.cpp</span><br><span class="line">…\3rdparty\llama.cpp\common\log.cpp</span><br><span class="line">…\3rdparty\llama.cpp\examples\imatrix\imatrix.cpp</span><br><span class="line">…\3rdparty\llama.cpp\examples\perplexity\perplexity.cpp</span><br><span class="line">… 代表bitnet项目的根目录。</span><br></pre></td></tr></table></figure><p>修复的内容是一样的，在头部添加一下引用即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br></pre></td></tr></table></figure><p>这样可以解决基本问题，但是我在运行时出现了以下报错</p><img src="image-20250524152618039.png" alt="image-20250524152618039" style="zoom: 67%;" /><p>这个问题在github文档中也有提到</p><img src="image-20250524152746798.png" alt="image-20250524152746798" style="zoom: 67%;" /><p>是因为无法在 Windows 的 conda 环境中使用 clang 进行构建导致的，根据文档中的提示进行修改，我使用的是PowerShell</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Import-Module <span class="string">&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot;</span> Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments <span class="string">&quot;-arch=x64 -host_arch=x64&quot;</span></span><br></pre></td></tr></table></figure><p>这里要注意目录文件中的 <strong>\Professional</strong> 要根据自己的Visual Studio版本进行修改，我的是Comuunity版本</p><p>同时这条命令可以分开执行，首先执行如下命令直接加载DevShell模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Import-Module <span class="string">&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot;</span></span><br></pre></td></tr></table></figure><img src="image-20250524153124051.png" alt="image-20250524153124051" /><p>然后执行如下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments <span class="string">&quot;-arch=x64 -host_arch=x64&quot;</span></span><br></pre></td></tr></table></figure><p>要注意，这里的 <strong>“3f0e31ad”</strong> 是 <strong>Instance ID</strong>，需要自己去查找，否则就会出现以下错误</p><img src="image-20250524153216797.png" alt="image-20250524153216797" /><p>查找过程非常简单，按照如下步骤即可：</p><p>首先安装VSSetup PowerShell模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Install-Module -Name VSSetup -Scope CurrentUser -Force</span><br></pre></td></tr></table></figure><img src="image-20250524153251877.png" alt="image-20250524153251877" /><p>安装后导入模块并查询</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Import-Module VSSetup</span><br><span class="line">Get-VSSetupInstance</span><br></pre></td></tr></table></figure><img src="image-20250524153425503.png" alt="image-20250524153425503" style="zoom:67%;" /><p>然后根据获取的InstanceID修改命令，运行即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Enter-VsDevShell 6d25e4c3 -SkipAutomaticLocation -DevCmdArguments <span class="string">&quot;-arch=x64 -host_arch=x64&quot;</span></span><br></pre></td></tr></table></figure><img src="image-20250524153549415.png" alt="image-20250524153549415" /><p>输入如下命令，可以验证此时可以使用clang</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clang -v</span><br></pre></td></tr></table></figure><img src="image-20250524153810794.png" alt="image-20250524153810794" /><p>最后执行编译指令，就可以通过编译了，这里可能需要稍等一小会儿才会编译完成。</p><img src="image-20250524153720330.png" alt="image-20250524153720330" /><h2 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h2><p>经过漫长的环境配置之后，终于可以测试了，执行以下代码，启动对话窗口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_inference.py -m models/BitNet-b1<span class="number">.58</span>-2B-4T/ggml-model-i2_s.gguf -p <span class="string">&quot;You are a helpful assistant&quot;</span> -cnv</span><br></pre></td></tr></table></figure><img src="image-20250524153924601.png" alt="image-20250524153924601" style="zoom:67%;" /><p>至此，我们就基本完成了本地部署BitNet的工作了，就像文章开头说过的，目前，本地跑的BitNet模型，并不是一个全精度模型，尺寸很小，所以可以流畅的运行在CPU环境下，且内存占用率极低，但实际测试大部分的对话它是理解错误的，也就是不是很可用，但这东西如果专门用来探索边缘计算的场景，经过调教之后，应该也能发挥很大的用处。</p>]]></content>
      
      
      <categories>
          
          <category> BitNet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BitNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>院校投递</title>
      <link href="/2024/12/15/%E9%99%A2%E6%A0%A1%E6%8A%95%E9%80%92/"/>
      <url>/2024/12/15/%E9%99%A2%E6%A0%A1%E6%8A%95%E9%80%92/</url>
      
        <content type="html"><![CDATA[<p>致成功推免的自己，既然选择远方，当不负青春，砥砺前行。青春由磨砺而出彩，人生因奋斗而升华！但行前路，不负韶华！每一个裂缝都是为透出光而努力。本文是个人推免院校投递情况，圆梦山大！！！为国家储人才，为民族图富强！学无止境，气有浩然！</p><table><thead><tr><th align="center">序号</th><th align="center">学校名称</th><th align="center">报考学院及专业</th><th align="center">状态</th><th align="center">是否投递</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><strong><span style="color:#CC0000;">东北大学</span></strong></td><td align="center">计算机科学与工程学院 计算机科学与技术</td><td align="center"><strong><span style="color:#CC0000;">进入复试，候补</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">2</td><td align="center">天津大学</td><td align="center">智能与计算学部 计算机科学与技术</td><td align="center"><strong><span style="color:#0080FF;">机测未通过</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">3</td><td align="center">重庆大学</td><td align="center">计算机学院 计算机科学与技术</td><td align="center"><strong><span style="color:#9933FF;">进入复试已拒绝</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">4</td><td align="center">山东大学</td><td align="center">人工智能学院 计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">5</td><td align="center">山东大学</td><td align="center">软件学院 人工智能</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">6</td><td align="center"><strong><span style="color:#CC0000;">山东大学</span></strong></td><td align="center">计算机科学与技术学院 计算机科学与技术(学硕)</td><td align="center"><strong><span style="color:#CC0000;">进入复试，上岸</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">7</td><td align="center">哈工深</td><td align="center">计算机科学与技术学院 计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">8</td><td align="center">中海洋</td><td align="center">信息科学与工程学部 计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">9</td><td align="center">大工</td><td align="center">计算机科学与技术学院  计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">10</td><td align="center">中山大学</td><td align="center">计算机学院 计算机技术(专硕)</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">11</td><td align="center">湖南大学</td><td align="center">信息科学与工程学院 计算机科学与技术</td><td align="center"><strong><span style="color:#FF66B2;">个人失误</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">12</td><td align="center">华南理工</td><td align="center">计算机科学与工程学院 计算机应用技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">13</td><td align="center">华南理工</td><td align="center">未来科技学院 人工智能</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">14</td><td align="center">西工大</td><td align="center">计算机学院 计算机科学与技术</td><td align="center"></td><td align="center">未投递</td></tr><tr><td align="center">15</td><td align="center">中山大学</td><td align="center">人工智能学院</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">16</td><td align="center">华东师范</td><td align="center">计算机科学与技术学院 计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">17</td><td align="center">国科大</td><td align="center">计算机网络信息中心 计算机应用技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">18</td><td align="center"><span style="color:#CC0000; font-weight:bold;">南理</span></td><td align="center">计算机科学与工程学院 计算机科学与技术</td><td align="center"><strong><span style="color:#CC0000;">拟录取</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">19</td><td align="center">南航</td><td align="center">计算机科学与技术学院&#x2F;人工智能学院&#x2F;软件学院 计算机科学与技术</td><td align="center"><strong><span style="color:#9933FF;">进入复试已拒绝</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">20</td><td align="center">西电</td><td align="center">计算机科学与技术学院  计算机科学与技术</td><td align="center"><strong>被拒</strong></td><td align="center">已投递</td></tr><tr><td align="center">21</td><td align="center">暨南大学</td><td align="center">暨南大学 信息科学技术学院 人工智能</td><td align="center"><strong><span style="color:#9933FF;">进入复试已拒绝</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">22</td><td align="center">上海大学</td><td align="center">计算机工程与科学学院 计算机科学与技术</td><td align="center"><strong><span style="color:#9933FF;">进入复试已拒绝</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">23</td><td align="center">武理</td><td align="center">计算机与信息技术学院 计算机科学与技术</td><td align="center"></td><td align="center">未投递</td></tr><tr><td align="center">24</td><td align="center">华东理工</td><td align="center">信息科学与工程学院 计算机科学与技术</td><td align="center"></td><td align="center">未投递</td></tr><tr><td align="center">25</td><td align="center"><strong><span style="color:#CC0000;">合工大</span></strong></td><td align="center">计算机与信息技术学院 计算机科学与技术</td><td align="center"><strong><span style="color:#CC0000;">拟录取</span></strong></td><td align="center">已投递</td></tr><tr><td align="center">26</td><td align="center">北交</td><td align="center">计算机与信息技术学院 计算机科学与技术</td><td align="center"></td><td align="center">未投递</td></tr><tr><td align="center">27</td><td align="center"><strong><span style="color:#CC0000;">西电</span></strong></td><td align="center">空间科学与技术学院 控制科学与工程</td><td align="center"><strong><span style="color:#CC0000;">进入复试，候补</span></strong></td><td align="center">已投递</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 个人杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推免 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/12/15/hello-world/"/>
      <url>/2024/12/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
